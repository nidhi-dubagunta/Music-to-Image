{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8oILUuS1Owq"
      },
      "source": [
        "Create TF-IDF Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "88VBi1PA1SRo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('spotify_and_clean_lyrics.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_features = pd.read_csv('x_unigram_features.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Unnamed: 0', 'aa', 'abandon', 'abide', 'ability', 'ablaze', 'able',\n",
            "       'aboard', 'absence', 'absent',\n",
            "       ...\n",
            "       'west coast rap', 'western swing', 'witch house', 'world.1',\n",
            "       'world fusion', 'worship.1', 'yacht rock', 'yoga', 'zen', 'zeuhl'],\n",
            "      dtype='object', length=5522)\n"
          ]
        }
      ],
      "source": [
        "#x_features = x_features.drop(['Unnamed: 0'], axis=1)\n",
        "x_feat_cols = x_features.columns\n",
        "print(x_feat_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#label encoder and decoder\n",
        "y_labels = pd.read_csv('y_label_categorized.csv')\n",
        "y_labels = y_labels.drop(['Unnamed: 0'], axis=1)\n",
        "\n",
        "# y_labels.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "consolidated_labels = []\n",
        "\n",
        "# Loop through each row in the DataFrame\n",
        "for index, row in y_labels.iterrows():\n",
        "    # Find the column with the non-zero value (1)\n",
        "    label = row.idxmax()\n",
        "    # Append the label to the list\n",
        "    consolidated_labels.append(label)\n",
        "\n",
        "# Create a new column 'label' in the DataFrame with the consolidated labels\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "consolidated_encoded = le.fit_transform(consolidated_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GhTWfBaH1e40"
      },
      "outputs": [],
      "source": [
        "df[df['processed_lyrics'].isnull()]\n",
        "#remove the rows with nans in df['clean_lyrics']\n",
        "df = df[df['processed_lyrics'].notnull()]\n",
        "df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'aa', 'abandon', 'abide', 'ability', 'ablaze', 'able',\n",
              "       'aboard', 'absence', 'absent',\n",
              "       ...\n",
              "       'west coast rap', 'western swing', 'witch house', 'world.1',\n",
              "       'world fusion', 'worship.1', 'yacht rock', 'yoga', 'zen', 'zeuhl'],\n",
              "      dtype='object', length=5522)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#drop Unnamed: 0 column\n",
        "# x_features = x_features.drop(['Unnamed: 0'], axis=1)\n",
        "x_feat_cols = x_features.columns\n",
        "x_feat_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KvRqjH1V1tXM"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def tfidf(df, ngram_range, max_features):\n",
        "    if(max_features == 0):\n",
        "        tfidf = TfidfVectorizer(ngram_range=ngram_range)\n",
        "    else:\n",
        "        tfidf = TfidfVectorizer(ngram_range=ngram_range, max_features=max_features)\n",
        "    tfidf_matrix = tfidf.fit_transform(df['clean_lyrics'])\n",
        "    return tfidf\n",
        "\n",
        "\n",
        "x_tfidf_unigram = tfidf(df, (1,1), 5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pTiaBJeR2V6q"
      },
      "outputs": [],
      "source": [
        "#use better-profanity to filter out profanity\n",
        "from better_profanity import profanity\n",
        "\n",
        "def filter_profanity(text):\n",
        "    profanity.load_censor_words()\n",
        "    text = profanity.censor(text, '')\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bl5MyFb32kct"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "import string\n",
        "def clean_text(text):\n",
        "    punct = '“’'\n",
        "    # remove numbers\n",
        "    text_nonum = re.sub(r'\\d+', '', text)\n",
        "    # remove punctuations and convert characters to lower case\n",
        "    text_nopunct = \"\"\n",
        "    for char in text_nonum:\n",
        "        if char not in string.punctuation and char not in punct:\n",
        "            text_nopunct += char.lower()\n",
        "        else:    \n",
        "            text_nopunct += \" \"\n",
        "    # substitute multiple whitespace with single whitespace\n",
        "    # Also, removes leading and trailing whitespaces\n",
        "    text_no_doublespace = re.sub('\\s+', ' ', text_nopunct).strip()\n",
        "    return text_no_doublespace\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA_5gsUa2lR5",
        "outputId": "547840bf-18e7-4567-fe10-d5b68f9ad30b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\nidhi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\nidhi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "import contractions\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def tokenize(text):\n",
        "    text = contractions.fix(text)\n",
        "    text = clean_text(text)\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DlHoE_362s2k"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "def stop_words_input(input_text):\n",
        "    \n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    filtered_text = []\n",
        "    for w in input_text: \n",
        "        w = w.strip()\n",
        "        if w not in stop_words and w != \"s\":\n",
        "            filtered_text.append(w)\n",
        "    return filtered_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "m4lGficB2viS"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def lemmatize(input_text):\n",
        "    lemma = []\n",
        "    for w in input_text:\n",
        "        #this is so jank\n",
        "        #ideally we would have to do POS tagging and then pass the actual tag to the function\n",
        "        w = lemmatizer.lemmatize(w,  'v')\n",
        "        w = lemmatizer.lemmatize(w,  'a')\n",
        "        w = lemmatizer.lemmatize(w,  'r')\n",
        "        lemma.append(lemmatizer.lemmatize(w,  'n'))\n",
        "    return lemma\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7SJsziO023Nm"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "def array_to_string(input_array):\n",
        "    words = set(nltk.corpus.words.words())\n",
        "\n",
        "    text = \"\"\n",
        "    for w in input_array:\n",
        "        #english language !\n",
        "        if(w in words):\n",
        "            text += w + \" \"\n",
        "    \n",
        "    return text\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nt5lu7UX25V_"
      },
      "outputs": [],
      "source": [
        "def process_text(text):\n",
        "    tokens = tokenize(text)\n",
        "    remove_stop = stop_words_input(tokens)\n",
        "    lemmatized = lemmatize(tokens) \n",
        "    text = array_to_string(tokens)\n",
        "    text = filter_profanity(text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVw0b3Ci27_W"
      },
      "source": [
        "Get Lyrics from Artist Name and Song Title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Kqya1_o73RhD"
      },
      "outputs": [],
      "source": [
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "\n",
        "def scrape_lyrics(artist, song):\n",
        "    GENIUS_API_TOKEN = 'osvPFwS1hvo1Y0oUI659YobdX11IWnS-B8-3uEw-SxaIllHqzmdBtCRoH8jzXlNh'\n",
        "    artist = re.sub('[^a-zA-Z0-9\\s]', '', artist)\n",
        "    song = re.sub('[^a-zA-Z0-9\\s]', '', song)\n",
        "\n",
        "    artistname = artist.replace(' ', '-')\n",
        "    songname = song.replace(' ', '-')\n",
        "    url = 'https://genius.com/' + artistname + '-' + songname + '-lyrics'\n",
        "    print(url)\n",
        "    page = requests.get(url)\n",
        "    html = BeautifulSoup(page.text, 'html.parser')\n",
        "    lyrics1 = html.find('div', class_='lyrics')\n",
        "    lyrics2 = html.find('div', class_='Lyrics__Container-sc-1ynbvzw-5 Dzxov')\n",
        "\n",
        "    \n",
        "    if lyrics1:\n",
        "        lyrics = lyrics1.get_text(separator=' ', strip=True)\n",
        "    elif lyrics2:\n",
        "        lyrics = lyrics2.get_text(separator=' ', strip=True)\n",
        "    elif lyrics1 == lyrics2:\n",
        "        return None\n",
        "    \n",
        "    #new line is missing from lyrics\n",
        "    return lyrics\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jz6NKoOk4WBl"
      },
      "outputs": [],
      "source": [
        "def get_model_word_features(lyrics):\n",
        "  processed_lyrics = process_text(lyrics)\n",
        "  lyric_list = []\n",
        "  lyric_list.append(processed_lyrics)\n",
        "\n",
        "  vectors = x_tfidf_unigram.transform(lyric_list)\n",
        "  feature_names = x_tfidf_unigram.get_feature_names_out()\n",
        "  dense = vectors.todense()\n",
        "  denselist = dense.tolist()\n",
        "  x_input = pd.DataFrame(denselist, columns=feature_names)\n",
        "  return x_input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yfRTQa23gA6"
      },
      "source": [
        "Get spotify features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP0HRXnI3XtY",
        "outputId": "b1e0ef93-0331-43d8-a20f-f6ff94fece73"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spotipy\n",
        "from spotipy.oauth2 import SpotifyClientCredentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mt_1t4mb34p3"
      },
      "outputs": [],
      "source": [
        "def get_spotify_features(track, artist):\n",
        "\n",
        "  client_id ='3699ed94db30435e8de48bab33770cab'\n",
        "  client_secret = 'a65a1e99bdee45b5a5d8699a38618c63'\n",
        "  client_credentials_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)\n",
        "  sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n",
        "\n",
        "  # Search for a track\n",
        "  track_name = track\n",
        "  artist_name = artist\n",
        "  results = sp.search(q='track:\"{}\" artist:\"{}\"'.format(track_name, artist_name), type='track', limit=1)\n",
        "\n",
        "  genres = pd.read_csv('dummy_genres.csv')\n",
        "  #dont know why rock-n-roll is here but not in x_features\n",
        "  genres = genres.drop(columns=['Unnamed: 0', 'rock-n-roll', 'sad', 'turntablism'])\n",
        "  genre_columns = genres.columns\n",
        "\n",
        "\n",
        "  result = sp.search(artist)\n",
        "  artist_uri = result['tracks']['items'][0]['artists'][0]['uri']\n",
        "\n",
        "  # Get the artist object\n",
        "  artist = sp.artist(artist_uri)\n",
        "\n",
        "  artist_genres = artist['genres']\n",
        "  print(artist_genres)\n",
        "  for genre in artist_genres:\n",
        "    if genre in genre_columns:\n",
        "      genres[genre] = 1.0\n",
        "\n",
        "  # Extract track details\n",
        "  if results['tracks']['items']:\n",
        "      track = results['tracks']['items'][0]\n",
        "      track_id = track['id']\n",
        "      track_name = track['name']\n",
        "      artist_name = track['artists'][0]['name']\n",
        "      audio_features = sp.audio_features([track_id])[0]\n",
        "      audio_feature_columns = ['valence_tags', 'arousal_tags', 'dominance_tags', 'danceability', 'energy_1', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n",
        "      # Create a DataFrame to store audio features\n",
        "      #dominance is not \n",
        "      df = pd.DataFrame({'track_name': [track_name],\n",
        "                        'artist_name': [artist_name],\n",
        "                         'dominance_tags': 5.310701476955552,\n",
        "                         'arousal_tags': 4.272828654445337,\n",
        "                        'danceability': [audio_features['danceability']],\n",
        "                        'energy_1': [audio_features['energy']],\n",
        "                        'loudness': [audio_features['loudness']],\n",
        "                        'speechiness': [audio_features['speechiness']],\n",
        "                        'acousticness': [audio_features['acousticness']],\n",
        "                        'instrumentalness': [audio_features['instrumentalness']],\n",
        "                        'liveness': [audio_features['liveness']],\n",
        "                        'valence': [audio_features['valence']],\n",
        "                         'valence_tags':[audio_features['valence']],\n",
        "                         'key': [audio_features['key']],\n",
        "                       'mode': [audio_features['mode']],\n",
        "                        'tempo': [audio_features['tempo']]})\n",
        "      \n",
        "      return pd.concat([df[audio_feature_columns], genres], axis=1)\n",
        "\n",
        "  else:\n",
        "      print('No matching track found.')\n",
        "      #return empty dataframe\n",
        "\n",
        "      return pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd6Eqcgr4Sdr"
      },
      "source": [
        "Pipeline Officially Starts!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KO4QflF0sLg",
        "outputId": "c5313ff0-169a-459e-98b2-503bf553ec34"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python39\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.2.2 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "c:\\Python39\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 1.2.2 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Load the pickled model from file\n",
        "with open('rf_binary_relevance.pkl', 'rb') as file:\n",
        "    cat_model = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['uplifting', 'pessimistic', 'romantic', 'playful', 'spiritual',\n",
              "       'introspective', 'dramatic', 'intense', 'nostalgic', 'dark', 'lively',\n",
              "       'calm', 'sophisticated', 'whimsical', 'sarcastic', 'atmospheric',\n",
              "       'energetic', 'naturalistic', 'sensual', 'philosophical', 'emotional',\n",
              "       'angry', 'delicate', 'melancholic', 'humorous', 'soothing',\n",
              "       'reflective', 'dissonant', 'mysterious'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_label_cols = y_labels.columns\n",
        "y_label_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_labels(prediction):\n",
        "    \n",
        "    prediction = prediction.todense()\n",
        "    prediction = prediction.tolist()\n",
        "    top_three = sorted(prediction[0], reverse=True)[:3]\n",
        "    top_three\n",
        "    #find indices of top 3 predictions\n",
        "    top_three_indices = []\n",
        "    print(prediction[0])\n",
        "    for i in top_three:\n",
        "        print(i)\n",
        "        if(i < 0.1):\n",
        "            break\n",
        "        top_three_indices.append(prediction[0].index(i))\n",
        "        prediction[0][prediction[0].index(i)] = -1\n",
        "    # print(top_three_indices)\n",
        "    # print(top_three)\n",
        "    #get labels of top 3 predictions\n",
        "    top_three_labels = []\n",
        "    for i in top_three_indices:\n",
        "        top_three_labels.append(y_label_cols[i])\n",
        "    return top_three_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import BERT_lyrics_params.pth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "while True:\n",
        "\n",
        "  artist_name = input(\"Enter Artist Name: \")\n",
        "  song_title = input(\"Enter Song Title: \")\n",
        "  lyrics = scrape_lyrics(artist_name, song_title)\n",
        "\n",
        "  if lyrics == None:\n",
        "    print(\"Couldn't find lyrics. Sorry!\")\n",
        "    continue\n",
        "  else:\n",
        "    x_text_features = get_model_word_features(lyrics)\n",
        "\n",
        "  \n",
        "  x_audio_features = get_spotify_features(song_title, artist_name)\n",
        "  x_audio_features\n",
        "\n",
        "  if(x_audio_features.empty):\n",
        "    print(\"Couldn't find spotify features. Sorry!\")\n",
        "\n",
        "  else:\n",
        "    x_complete = pd.concat([x_text_features, x_audio_features], axis=1)\n",
        "    #set column names to x_feat_cols\n",
        "    # print(x_complete.columns)\n",
        "   # x_complete.insert(0, 'Unnamed: 0', 45000)\n",
        "    #remove first element from x_feat_cols\n",
        "    x_feat_cols_new = x_feat_cols[1:]\n",
        "    x_complete.columns = x_feat_cols_new\n",
        "    \n",
        "    x_complete\n",
        "    #add column at position 0 called Unnamed: 0 with value 0\n",
        "    \n",
        "    prediction = cat_model.predict_proba(x_complete)\n",
        "    print(prediction)\n",
        "    #print top 3 predictions and their probabilities\n",
        "   \n",
        "    print(get_labels(prediction))\n",
        "    # `#decode prediction\n",
        "    # print(\"The song is: \", prediction[0])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(x_feat_cols)):\n",
        "    print(x_feat_cols[i], x_complete.columns[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['abandon', 'abide', 'ability', 'ablaze', 'able', 'aboard', 'absence',\n",
              "       'absent', 'absolute', 'absolutely',\n",
              "       ...\n",
              "       'west coast rap', 'western swing', 'witch house', 'world.1',\n",
              "       'world fusion', 'worship.1', 'yacht rock', 'yoga', 'zen', 'zeuhl'],\n",
              "      dtype='object', length=5520)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_feat_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
